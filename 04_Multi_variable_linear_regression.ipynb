{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04-Multi variable linear regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMTdLj+WjEmJ+OZk8qRet+1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bingjun5249/MLwith.Tensorflow/blob/main/04_Multi_variable_linear_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ruj0OEPKaMok"
      },
      "source": [
        "# 다변수 선형 회귀\r\n",
        "\r\n",
        "변수가 여러 개인 cost 함수\r\n",
        "\r\n",
        " -행렬을 사용해 연산한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_UIMDDtZXcO",
        "outputId": "b27e9269-745f-4ff4-817d-2db209c3b40b"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "# data and label\r\n",
        "x1 = [73., 93., 89., 96., 73.]\r\n",
        "x2 = [80., 88., 91., 98., 66.]\r\n",
        "x3 = [75., 93., 90., 100., 70.]\r\n",
        "Y = [152., 185., 180., 196., 142.]\r\n",
        "\r\n",
        "# random weights\r\n",
        "w1 = tf.Variable(tf.random.normal([1]))\r\n",
        "w2 = tf.Variable(tf.random.normal([1]))\r\n",
        "w3 = tf.Variable(tf.random.normal([1]))\r\n",
        "b = tf.Variable(tf.random.normal([1]))\r\n",
        "\r\n",
        "learning_rate = 0.000001\r\n",
        "\r\n",
        "for i in range(1000+1):\r\n",
        "  #tf.GradientTape() to record the gradient of the cost function\r\n",
        "  with tf.GradientTape() as tape:\r\n",
        "    hypothesis = w1 * x1 + w2 * x2 + w3 * x3 + b\r\n",
        "    cost = tf.reduce_mean(tf.square(hypothesis - Y))\r\n",
        "  # calculates the gradients of the cost\r\n",
        "  w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])\r\n",
        "\r\n",
        "  #update w1,w2,w3 and b\r\n",
        "  w1.assign_sub(learning_rate * w1_grad)\r\n",
        "  w2.assign_sub(learning_rate * w2_grad)\r\n",
        "  w3.assign_sub(learning_rate * w3_grad)\r\n",
        "  b.assign_sub(learning_rate * b_grad)\r\n",
        "\r\n",
        "  if i % 50 == 0:\r\n",
        "    print(\"{:5} | {:12.4f}\".format(i, cost.numpy()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    0 |  126399.7891\n",
            "   50 |    1432.1160\n",
            "  100 |      45.3924\n",
            "  150 |      29.9276\n",
            "  200 |      29.6786\n",
            "  250 |      29.5987\n",
            "  300 |      29.5209\n",
            "  350 |      29.4432\n",
            "  400 |      29.3658\n",
            "  450 |      29.2886\n",
            "  500 |      29.2116\n",
            "  550 |      29.1348\n",
            "  600 |      29.0583\n",
            "  650 |      28.9819\n",
            "  700 |      28.9058\n",
            "  750 |      28.8299\n",
            "  800 |      28.7542\n",
            "  850 |      28.6787\n",
            "  900 |      28.6033\n",
            "  950 |      28.5282\n",
            " 1000 |      28.4532\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MERR0VFrgNJq"
      },
      "source": [
        "행렬을 사용해 간략화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vj3YdVZ4avmA",
        "outputId": "72aba5fc-fb2a-4a60-c6d6-a60076a087de"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "data = np.array([\r\n",
        "    # X1,  X2,  X3,   Y\r\n",
        "    [ 73., 80., 75., 152.],\r\n",
        "    [ 93., 88., 93., 185.],\r\n",
        "    [ 89., 91., 90., 180.],\r\n",
        "    [ 96., 98.,100., 196.],\r\n",
        "    [ 73., 66., 70., 142.]\r\n",
        "], dtype=np.float32)\r\n",
        "\r\n",
        "# slice data\r\n",
        "X = data[:, :-1]\r\n",
        "Y = data[:, [-1]]\r\n",
        "\r\n",
        "W = tf.Variable(tf.random.normal([3, 1]))\r\n",
        "b = tf.Variable(tf.random.normal([1]))\r\n",
        "\r\n",
        "learning_rate = 0.000001\r\n",
        "\r\n",
        "# hpothesis, prediction function\r\n",
        "def predict(X):\r\n",
        "  return tf.matmul(X, W) + b  # 행렬의 곱셈\r\n",
        "\r\n",
        "n_epochs = 2000\r\n",
        "for i in range(n_epochs+1):\r\n",
        "  # record the gradient of the cost function\r\n",
        "  with tf.GradientTape() as tape:\r\n",
        "    cost = tf.reduce_mean((tf.square(predict(X) - Y)))\r\n",
        "\r\n",
        "  #calculates the gradients of the Loss\r\n",
        "  W_grad, b_grad = tape.gradient(cost, [W, b])\r\n",
        "\r\n",
        "  #updates parameters (W and b)\r\n",
        "  W.assign_sub(learning_rate * W_grad)\r\n",
        "  b.assign_sub(learning_rate * b_grad)\r\n",
        "\r\n",
        "  if i % 100 == 0:\r\n",
        "    print(\"{:5} | {:10.4f}\".format(i, cost.numpy()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    0 | 37230.8828\n",
            "  100 |     7.9256\n",
            "  200 |     3.3399\n",
            "  300 |     3.3365\n",
            "  400 |     3.3337\n",
            "  500 |     3.3310\n",
            "  600 |     3.3282\n",
            "  700 |     3.3254\n",
            "  800 |     3.3226\n",
            "  900 |     3.3199\n",
            " 1000 |     3.3171\n",
            " 1100 |     3.3144\n",
            " 1200 |     3.3116\n",
            " 1300 |     3.3089\n",
            " 1400 |     3.3062\n",
            " 1500 |     3.3035\n",
            " 1600 |     3.3008\n",
            " 1700 |     3.2982\n",
            " 1800 |     3.2955\n",
            " 1900 |     3.2929\n",
            " 2000 |     3.2902\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}